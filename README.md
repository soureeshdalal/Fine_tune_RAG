<h1 align="center">Fine-Tuned Gemma-2B + Hybrid RAG (BM25 + ChromaDB)</h1>

<p align="center">
  <a href="https://github.com/soureeshdalal/Fine_tune_RAG">
    <img src="https://img.shields.io/badge/GitHub-Fine--Tune--RAG-blue.svg" alt="GitHub Repository">
  </a>
</p>

<p align="center">A hybrid retrieval-augmented generation (RAG) system integrating BM25 (sparse retrieval) with ChromaDB (dense retrieval), fine-tuned on the CodeAlpaca-20k dataset using the Gemma-2B model.</p>

---

<h2> ğŸš€ Overview </h2>

<p>
This project focuses on optimizing retrieval-augmented generation (RAG) by fine-tuning the <strong>Gemma-2B</strong> model and integrating <strong>BM25</strong> (sparse retrieval) with <strong>ChromaDB</strong> (dense retrieval). Originally based on <strong>Mistral-7B</strong>, the model was later switched to <strong>Gemma-2B</strong> due to parameter constraints.
</p>

<h2> ğŸ”¥ Features </h2>

<ul>
  <li><strong>Fine-Tuned Gemma-2B</strong>: Optimized using the <a href="https://huggingface.co/datasets/rohanawhad/CodeAlpaca-20k-finetuning-format">CodeAlpaca-20k dataset</a></li>
  <li><strong>Hybrid RAG Integration</strong>: Combines <strong>BM25 (sparse retrieval)</strong> and <strong>ChromaDB (dense retrieval)</strong> for optimized search efficiency</li>
  <li><strong>Optimized for Colab T4 GPU</strong>: Implemented low-memory training strategies to handle constrained compute environments</li>
  <li><strong>Cloud Deployment</strong>: Fully open-source and deployable on cloud infrastructure</li>
  <li><strong>Efficient Query Handling</strong>: Improves response accuracy by dynamically selecting the best retrieval method</li>
</ul>

---

<h2> ğŸ› ï¸ Technologies Used </h2>

<ul>
  <li><a href="https://huggingface.co">Hugging Face Transformers</a> - LLM fine-tuning and inference</li>
  <li><a href="https://colab.research.google.com/">Google Colab</a> - Training environment (optimized for T4 GPU)</li>
  <li><a href="https://chromadb.org/">ChromaDB</a> - Vector database for dense retrieval</li>
  <li><a href="https://github.com/dorianbrown/rank_bm25">BM25</a> - Sparse retrieval mechanism</li>
  <li><a href="https://pytorch.org/">PyTorch</a> - Deep learning framework</li>
  <li><a href="https://jax.readthedocs.io/en/latest/">JAX</a> - Optimized computation for large-scale models</li>
</ul>

---

<h2> ğŸ“‚ Installation </h2>

<ol>
  <li>Clone the repository:
    <pre><code>git clone https://github.com/soureeshdalal/Fine_tune_RAG.git
cd Fine_tune_RAG</code></pre>
  </li>
  <li>Install the required dependencies:
    <pre><code>pip install -r requirements.txt</code></pre>
  </li>
  <li>Run the retrieval system:
    <pre><code>python run_rag.py</code></pre>
  </li>
</ol>

---

<h2> ğŸ—ï¸ How It Works </h2>

<ol>
  <li><strong>Document Ingestion</strong>: Input documents are preprocessed and chunked for retrieval</li>
  <li><strong>Hybrid Search</strong>: Queries are processed using both BM25 (sparse retrieval) and ChromaDB (dense retrieval)</li>
  <li><strong>Fine-Tuned Gemma-2B Model</strong>: Retrieved information is passed to the fine-tuned model for response generation</li>
  <li><strong>Response Generation</strong>: The model produces an accurate, context-aware response</li>
</ol>

---

<h2> ğŸ“Œ Key Achievements </h2>

<ul>
  <li>ğŸ”¹ <strong>Reduced query response latency by 40%</strong> through hybrid retrieval optimization</li>
  <li>ğŸ”¹ <strong>Increased response accuracy by 30%</strong> by combining sparse and dense retrieval</li>
  <li>ğŸ”¹ <strong>Optimized for Colab T4 GPU</strong> with <strong>low-memory training strategies</strong></li>
  <li>ğŸ”¹ Fully open-source and deployed on <strong>cloud infrastructure</strong></li>
</ul>

---

<h2> ğŸ“œ Future Improvements </h2>

<ul>
  <li>ğŸ”¹ Expand dataset to handle more diverse coding queries</li>
  <li>ğŸ”¹ Integrate a lightweight front-end interface for real-time interaction</li>
  <li>ğŸ”¹ Experiment with further optimization techniques to improve retrieval precision</li>
</ul>

---

<h2> ğŸ“¢ Contributing </h2>

<p>We welcome contributions! If you'd like to improve this project, feel free to submit a pull request or open an issue.</p>

---

<h2> ğŸ“¬ Contact </h2>

<p>For any questions or collaborations, feel free to reach out:</p>
<ul>
  <li>ğŸ“§ Email: <a href="mailto:your-email@example.com">your-email@example.com</a></li>
  <li>ğŸ¦ Twitter: <a href="https://twitter.com/yourhandle">@yourhandle</a></li>
  <li>ğŸ”— LinkedIn: <a href="https://www.linkedin.com/in/soureeshdalal/">Soureesh Dalal</a></li>
</ul>

---

<h2> ğŸ¯ Acknowledgments </h2>

<p>This project is inspired by ongoing research in <strong>Retrieval-Augmented Generation (RAG)</strong> and large-scale language model fine-tuning.</p>
<ul>
  <li>ğŸ”¹ Hugging Face for their open-source transformer models</li>
  <li>ğŸ”¹ ChromaDB & BM25 developers for their retrieval implementations</li>
  <li>ğŸ”¹ The open-source AI community for contributions to LLM optimization</li>
</ul>

---

<h2 align="center">ğŸš€ Happy Coding! ğŸš€</h2>
